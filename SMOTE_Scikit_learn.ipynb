{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhGdAbp8IGJXZQI3owYQEo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jagvgithub/Classification_Regression_models/blob/main/SMOTE_Scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Técnicas de remuestreo SMOTE (Synthetic Minority Over-sampling Technique) Scikit-learn"
      ],
      "metadata": {
        "id": "MARt9rlwKx3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. SMOTENC (SMOTE for Continuous and Categorical features)\n",
        "\n",
        "_Definición:_\n",
        "\n",
        "SMOTENC es una variante del algoritmo SMOTE que maneja tanto características continuas como categóricas. Aplica SMOTE en las características continuas y genera muestras sintéticas considerando las características categóricas.\n",
        "\n",
        "_Ventajas:_\n",
        "\n",
        "Maneja datasets mixtos con características continuas y categóricas.\n",
        "Genera muestras sintéticas que respetan la naturaleza de las características categóricas.\n",
        "\n",
        "_Desventajas:_\n",
        "\n",
        "Puede ser computacionalmente costoso en datasets grandes.\n",
        "Si no se seleccionan correctamente las características categóricas, puede introducir ruido.\n",
        "\n",
        "# 2. SMOTEN (SMOTE for Nominal features)\n",
        "\n",
        "_Definición:_\n",
        "\n",
        "SMOTEN está diseñado específicamente para datasets con características categóricas. Genera nuevas muestras sintéticas basadas en las frecuencias y combinaciones de las categorías existentes en el dataset.\n",
        "\n",
        "_Ventajas:_\n",
        "\n",
        "Optimizado para manejar solo características categóricas.\n",
        "Simple y eficiente en datasets categóricos.\n",
        "Desventajas:\n",
        "\n",
        "No es adecuado para datasets con características continuas.\n",
        "Puede ser menos efectivo si las combinaciones de categorías son muy diversas o raras.\n",
        "\n",
        "# 3. BorderlineSMOTE\n",
        "\n",
        "_Definición:_\n",
        "\n",
        "BorderlineSMOTE es una variante de SMOTE que se enfoca en los ejemplos cercanos a la frontera de decisión entre las clases. Genera nuevas muestras sintéticas solo para los ejemplos que están cerca de los límites de las clases.\n",
        "\n",
        "_Ventajas:_\n",
        "\n",
        "Mejora la precisión en las áreas críticas cercanas a las fronteras de decisión.\n",
        "Reduce el riesgo de overfitting en regiones no críticas.\n",
        "\n",
        "_Desventajas:_\n",
        "\n",
        "Puede ser menos efectivo en datasets donde las fronteras de decisión no son claras.\n",
        "Todavía puede generar ruido si las fronteras son muy complejas.\n",
        "\n",
        "# 4. SVMSMOTE\n",
        "\n",
        "_Definición:_\n",
        "\n",
        "SVMSMOTE utiliza una máquina de soporte vectorial (SVM) para identificar los ejemplos frontera y luego aplica SMOTE para generar ejemplos sintéticos cerca de estas fronteras.\n",
        "\n",
        "_Ventajas:_\n",
        "\n",
        "Enfoca el sobremuestreo en las áreas más difíciles de clasificar.\n",
        "Aprovecha la potencia de SVM para identificar las fronteras de decisión.\n",
        "\n",
        "Desventajas:\n",
        "\n",
        "Más complejo y computacionalmente costoso que otros métodos.\n",
        "Requiere ajuste de parámetros del SVM, lo cual puede ser complicado.\n",
        "\n",
        "# 5. ADASYN (Adaptive Synthetic Sampling)\n",
        "\n",
        "_Definición:_\n",
        "\n",
        "ADASYN ajusta la cantidad de ejemplos sintéticos generados para cada muestra minoritaria según la dificultad de la clasificación. Genera más ejemplos sintéticos para las muestras que son más difíciles de clasificar.\n",
        "\n",
        "_Ventajas:_\n",
        "\n",
        "Adapta dinámicamente la generación de muestras sintéticas según la necesidad.\n",
        "Mejora la precisión en áreas donde la clasificación es más difícil.\n",
        "Desventajas:\n",
        "\n",
        "Puede ser computacionalmente costoso.\n",
        "Puede introducir ruido si se generan demasiadas muestras sintéticas en áreas complejas.\n",
        "# 6. KMeansSMOTE\n",
        "\n",
        "_Definición:_\n",
        "\n",
        "KMeansSMOTE aplica un agrupamiento (clustering) usando K-Means antes de aplicar SMOTE. Esto permite generar ejemplos sintéticos dentro de los clústeres identificados.\n",
        "\n",
        "_Ventajas:_\n",
        "\n",
        "Mejora la distribución de los datos sintéticos.\n",
        "Puede reducir el riesgo de overfitting al generar muestras dentro de clústeres más naturales.\n",
        "\n",
        "_Desventajas:_\n",
        "\n",
        "Requiere determinar el número adecuado de clústeres.\n",
        "Puede ser computacionalmente costoso debido al paso de clustering."
      ],
      "metadata": {
        "id": "pMbfQdyxLyUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Principales inconvenientes del SMOTE\n",
        "\n",
        "_Overfitting (Sobreajuste):_\n",
        "\n",
        "Al generar demasiadas muestras sintéticas de la clase minoritaria, se puede provocar que el modelo se ajuste demasiado a los datos de entrenamiento. Esto significa que el modelo podría aprender patrones específicos del conjunto de entrenamiento que no generalizan bien a datos nuevos o no vistos, lo que reduce la capacidad predictiva del modelo en situaciones reales.\n",
        "\n",
        "_Ruido:_\n",
        "\n",
        "Al crear muestras sintéticas, especialmente con métodos más agresivos, existe el riesgo de introducir ruido en los datos. Las muestras sintéticas pueden no representar correctamente la distribución real de la clase minoritaria, lo que podría llevar a decisiones erróneas por parte del modelo.\n",
        "\n",
        "_Computational Complexity (Complejidad Computacional):_\n",
        "\n",
        "El aumento significativo del tamaño del conjunto de datos debido a la generación de muchas muestras sintéticas puede incrementar la carga computacional para el entrenamiento del modelo. Esto puede resultar en tiempos de entrenamiento más largos y mayores requerimientos de memoria.\n",
        "\n",
        "\n",
        "_Distorsión de la Distribución de Datos:_\n",
        "\n",
        "Crear muchas muestras sintéticas puede distorsionar la distribución original de los datos, especialmente si las muestras sintéticas no capturan correctamente las características intrínsecas de la clase minoritaria. Esto puede llevar a un modelo que no sea representativo del problema real.\n",
        "\n",
        "_False Confidence (Confianza Falsa):_\n",
        "\n",
        "Al tener un conjunto de datos balanceado de manera sintética, el modelo podría mostrar una alta precisión en el conjunto de entrenamiento y validación debido a la similitud entre las muestras sintéticas y las originales. Sin embargo, en un entorno de producción con datos reales desbalanceados, el rendimiento del modelo puede no ser tan bueno como se esperaba.\n",
        "\n",
        "\n",
        "# Mitigación de los Riesgos\n",
        "\n",
        "Para mitigar estos riesgos, se pueden seguir algunas estrategias:\n",
        "\n",
        "_Uso de Métodos de Sobremuestreo con Precaución:_\n",
        "\n",
        "En lugar de igualar completamente las clases, se puede optar por reducir el desbalance en lugar de eliminarlo por completo. Esto ayuda a mantener una representación más realista de los datos.\n",
        "\n",
        "_Combinación de Sobremuestreo y Submuestreo:_\n",
        "\n",
        "Una combinación de técnicas de sobremuestreo para la clase minoritaria y submuestreo para la clase mayoritaria puede ayudar a alcanzar un equilibrio sin sobreajustar el modelo.\n",
        "\n",
        "_Validación Cruzada:_\n",
        "\n",
        "Utilizar validación cruzada durante el entrenamiento del modelo puede ayudar a asegurarse de que el modelo generaliza bien y no está sobreajustado a las muestras sintéticas.\n",
        "\n",
        "_Ajuste de Hiperparámetros:_\n",
        "\n",
        "Realizar una búsqueda de hiperparámetros para optimizar el rendimiento del modelo puede ayudar a encontrar el balance adecuado entre el ajuste a los datos de entrenamiento y la capacidad de generalización.\n",
        "\n",
        "_Evaluación con Métricas Apropiadas:_\n",
        "\n",
        "Utilizar métricas de evaluación adecuadas para problemas de clasificación desbalanceada, como la matriz de confusión, la precisión, la sensibilidad, la especificidad y el F1-score, en lugar de solo la precisión global.\n",
        "\n",
        "Implementar estas estrategias puede ayudar a reducir los riesgos asociados con el balanceo extremo de clases y mejorar la robustez del modelo en situaciones del mundo real.\n",
        "\n"
      ],
      "metadata": {
        "id": "7HhtKgM4C5d4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwvUM8knKEpm",
        "outputId": "e7c36b59-31e3-4a04-e2fa-52a90b62044f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balance inicial de clases:\n",
            "columna_binaria\n",
            "0    83\n",
            "1    17\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Balance de clases después de aplicar SMOTENC:\n",
            "columna_binaria\n",
            "0    83\n",
            "1    83\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Balance de clases después de aplicar SMOTEN:\n",
            "columna_binaria\n",
            "0    83\n",
            "1    83\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Balance de clases después de aplicar BorderlineSMOTE:\n",
            "columna_binaria\n",
            "0    83\n",
            "1    83\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Balance de clases después de aplicar SVMSMOTE:\n",
            "columna_binaria\n",
            "0    83\n",
            "1    53\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Balance de clases después de aplicar ADASYN:\n",
            "columna_binaria\n",
            "1    84\n",
            "0    83\n",
            "Name: count, dtype: int64\n",
            "Error con KMeansSMOTE: No clusters found with sufficient samples of class 1. Try lowering the cluster_balance_threshold or increasing the number of clusters.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTENC, SMOTEN, BorderlineSMOTE, SVMSMOTE, ADASYN, KMeansSMOTE\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Definir el número de filas\n",
        "num_filas = 100\n",
        "\n",
        "# Crear un DataFrame con características continuas y categóricas\n",
        "data = {\n",
        "    'columna_1': np.random.rand(num_filas),\n",
        "    'columna_2': np.random.randint(0, 3, num_filas),  # Característica categórica\n",
        "    'columna_3': np.random.rand(num_filas),\n",
        "    'columna_4': np.random.randint(0, 3, num_filas),  # Característica categórica\n",
        "    'columna_5': np.random.rand(num_filas),\n",
        "    'columna_6': np.random.rand(num_filas),\n",
        "    'columna_7': np.random.rand(num_filas),\n",
        "    'columna_8': np.random.rand(num_filas),\n",
        "    'columna_9': np.random.rand(num_filas)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Crear una columna binaria con clase minoritaria (15% 1s y 85% 0s)\n",
        "df['columna_binaria'] = np.where(np.random.rand(num_filas) < 0.15, 1, 0)\n",
        "\n",
        "# Mostrar el balance inicial de clases\n",
        "print(\"Balance inicial de clases:\")\n",
        "print(df['columna_binaria'].value_counts())\n",
        "\n",
        "# Separar las características y la variable objetivo\n",
        "X = df.drop('columna_binaria', axis=1)\n",
        "y = df['columna_binaria']\n",
        "\n",
        "# Lista de métodos SMOTE a aplicar\n",
        "smote_methods = {\n",
        "    'SMOTENC': SMOTENC(categorical_features=[1, 3], random_state=42),\n",
        "    'SMOTEN': SMOTEN(random_state=42),\n",
        "    'BorderlineSMOTE': BorderlineSMOTE(random_state=42),\n",
        "    'SVMSMOTE': SVMSMOTE(random_state=42),\n",
        "    'ADASYN': ADASYN(random_state=42),\n",
        "    'KMeansSMOTE': KMeansSMOTE(kmeans_estimator=KMeans(n_clusters=10, random_state=42), random_state=42)\n",
        "}\n",
        "\n",
        "# Diccionario para almacenar los DataFrames resultantes\n",
        "df_resampled = {}\n",
        "\n",
        "# Aplicar cada método de SMOTE y almacenar los DataFrames resultantes\n",
        "for method_name, smote_method in smote_methods.items():\n",
        "    try:\n",
        "        X_res, y_res = smote_method.fit_resample(X, y)\n",
        "        df_res = pd.DataFrame(X_res, columns=X.columns)\n",
        "        df_res['columna_binaria'] = y_res\n",
        "        df_resampled[method_name] = df_res\n",
        "        print(f\"\\nBalance de clases después de aplicar {method_name}:\")\n",
        "        print(df_res['columna_binaria'].value_counts())\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error con {method_name}: {e}\")\n",
        "\n",
        "# Acceder a los DataFrames resultantes\n",
        "df_smote_nc = df_resampled.get('SMOTENC')\n",
        "df_smote_n = df_resampled.get('SMOTEN')\n",
        "df_borderline_smote = df_resampled.get('BorderlineSMOTE')\n",
        "df_svmsmote = df_resampled.get('SVMSMOTE')\n",
        "df_adasyn = df_resampled.get('ADASYN')\n",
        "df_kmeans_smote = df_resampled.get('KMeansSMOTE')"
      ]
    }
  ]
}